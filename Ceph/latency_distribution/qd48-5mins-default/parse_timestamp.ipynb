{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "utc=pytz.UTC # work with offset-naive and offset-aware datetimes\n",
    "\n",
    "f1='dump_time_stamps_vec.csv'\n",
    "data1=read_csv(f1, skiprows=10000, parse_dates=True, squeeze=True, sep=',', header=None)\n",
    "datalen = len(data1.values)\n",
    "\n",
    "def check_bracket(str):\n",
    "    if str == None:\n",
    "        return \"0\"\n",
    "    if str.startswith('['):\n",
    "        return str[1:]\n",
    "    if str.endswith(']'):\n",
    "        return str[:-1]\n",
    "    return str\n",
    "\n",
    "# for bluestore latency\n",
    "x_bs_lat = []\n",
    "y_bs_lat = []\n",
    "\n",
    "\n",
    "# process the time stamps\n",
    "for i in range(datalen-1):\n",
    "    # simple writes\n",
    "    #if len(data1.values[i,:]) == len(data1.values[i+1,:]) and data1.values[i,2] == 'simple_s':\n",
    "    if data1.values[i,2] == 'simple_s':\n",
    "        # for first ctx\n",
    "        ctr_ctx1 = parser.parse(check_bracket(data1.values[i,1])).replace(tzinfo=utc)\n",
    "        simple_s1 = parser.parse(check_bracket(data1.values[i,3])).replace(tzinfo=utc)\n",
    "        aio_done1 = parser.parse(check_bracket(data1.values[i,5])).replace(tzinfo=utc)\n",
    "        flush_cmt_s1 = parser.parse(check_bracket(data1.values[i,7])).replace(tzinfo=utc)\n",
    "        flush_cmt_e1 = parser.parse(check_bracket(data1.values[i,9])).replace(tzinfo=utc)\n",
    "        simple_e1 = parser.parse(check_bracket(data1.values[i,11])).replace(tzinfo=utc)\n",
    "        # for second ctx\n",
    "        '''ctr_ctx2 = parser.parse(check_bracket(data1.values[i+1,1]))\n",
    "        simple_s2 = parser.parse(check_bracket(data1.values[i+1,3]))\n",
    "        aio_done2 = parser.parse(check_bracket(data1.values[i+1,5]))\n",
    "        flush_cmt_s2 = parser.parse(check_bracket(data1.values[i+1,7]))\n",
    "        flush_cmt_e2 = parser.parse(check_bracket(data1.values[i+1,9]))\n",
    "        simple_e2 = parser.parse(check_bracket(data1.values[i+1,11]))'''\n",
    "        # sanity check of timestamps\n",
    "        if simple_s1 < ctr_ctx1 or aio_done1 < simple_s1 or flush_cmt_s1 < aio_done1 or flush_cmt_e1 < flush_cmt_s1 or simple_e1 < flush_cmt_e1:\n",
    "            print(\"simple writes timestamp order is incorrect\")\n",
    "        # bluestore latency\n",
    "        bluestore_lat_simple = simple_e1 - simple_s1\n",
    "        x_bs_lat.append(simple_s1)\n",
    "        y_bs_lat.append(bluestore_lat_simple.total_seconds())\n",
    "        \n",
    "        # spikes\n",
    "        if bluestore_lat_simple.total_seconds() > 0.05:\n",
    "            print(\"bluestore_lat_simple\",bluestore_lat_simple.total_seconds(),\", simple_start\",check_bracket(data1.values[i,3]))\n",
    "\n",
    "    # deferred writes\n",
    "    elif data1.values[i,2] == 'deferred_s':\n",
    "        ctr_ctx1 = parser.parse(check_bracket(data1.values[i,1])).replace(tzinfo=utc)\n",
    "        deferred_s1 = parser.parse(check_bracket(data1.values[i,3])).replace(tzinfo=utc)\n",
    "        flush_cmt_s1 = parser.parse(check_bracket(data1.values[i,5])).replace(tzinfo=utc)\n",
    "        flush_cmt_e1 = parser.parse(check_bracket(data1.values[i,7])).replace(tzinfo=utc)\n",
    "        deferred_e1 = parser.parse(check_bracket(data1.values[i,9])).replace(tzinfo=utc)\n",
    "        # sanity check of timestamps\n",
    "        if deferred_s1 < ctr_ctx1 or flush_cmt_s1 < deferred_s1 or flush_cmt_e1 < flush_cmt_s1 or deferred_e1 < flush_cmt_e1:\n",
    "            print(\"deferred writes timestamp order is incorrect\")\n",
    "        # bluestore latency\n",
    "        bluestore_lat_deferred = deferred_e1 - deferred_s1\n",
    "        x_bs_lat.append(deferred_s1)\n",
    "        y_bs_lat.append(bluestore_lat_deferred.total_seconds())\n",
    "        \n",
    "        # spikes\n",
    "        if bluestore_lat_deferred.total_seconds() > 0.05:\n",
    "            print(\"bluestore_lat_deferred\",bluestore_lat_deferred.total_seconds(),\", deferred_start\",check_bracket(data1.values[i,3]))\n",
    "\n",
    "if2='flush_job_timestamps.csv'     # Compaction for L0\n",
    "if3='compact_job_timestamps.csv'   # Compaction for other levels\n",
    "id2=read_csv(if2, parse_dates=True, squeeze=True, sep=',', header=None)\n",
    "id3=read_csv(if3, parse_dates=True, squeeze=True, sep=',', header=None)\n",
    "\n",
    "id2len = len(id2.values)\n",
    "id3len = len(id3.values)\n",
    "\n",
    "x2_compact = [] # flush(L0) timestamps\n",
    "x3_compact = [] # compact(>= L1) timestamps\n",
    "y2_compact = [] # dummp y value\n",
    "y3_compact = [] # dummp y value\n",
    "w2_compact = [] # durations(width of compaction)\n",
    "w3_compact = [] # durations(width of compaction)\n",
    "\n",
    "for i in range(id2len):\n",
    "    x2_compact.append((parser.parse(id2.values[i,1])-timedelta(hours=5)).replace(tzinfo=utc))\n",
    "    w2_compact.append(id2.values[i,5]/1000000)\n",
    "    y2_compact.append(0.05)\n",
    "for i in range(id3len):\n",
    "    x3_compact.append((parser.parse(id3.values[i,1])-timedelta(hours=5)).replace(tzinfo=utc))\n",
    "    w3_compact.append(id3.values[i,5]/1000000)\n",
    "    y3_compact.append(0.05)\n",
    "\n",
    "\n",
    "plt.plot(x2_compact, y2_compact, label='4096 KiB',marker='^', c='g', linestyle='')\n",
    "plt.plot(x3_compact, y3_compact, label='4096 KiB',marker='d', c='r', linestyle='')\n",
    "         \n",
    "plt.plot(x_bs_lat, y_bs_lat, label='bluestore')\n",
    "#ax.set(xlabel='time stamps', ylabel='latency [secs]', title='BlueStore Latency Time Series')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#CDF\n",
    "def plot_pdf_and_cdf(write_times, writes_durations, spike_times):\n",
    "    number_of_writes = len(writes_durations)\n",
    "    number_of_writes_before_spike = []\n",
    "    write_time_index = 0\n",
    "    for spike_time in spike_times:\n",
    "        count = 0\n",
    "        while write_time_index < len(write_times) and write_times[write_time_index] < spike_time:\n",
    "            write_time_index += 1\n",
    "            count += 1\n",
    "        number_of_writes_before_spike.append(count)\n",
    "    pdf = number_of_writes_before_spike / number_of_writes\n",
    "    cdf = np.cumsum(pdf)\n",
    "    plt.plot(spike_times, pdf, label='pdf')\n",
    "    plt.plot(spike_times, cdf, label='cdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_pdf_and_cdf(x_bs_lat, y_bs_lat, x2_compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (qd48-5mins-default)",
   "language": "python",
   "name": "pycharm-1d71e9c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}